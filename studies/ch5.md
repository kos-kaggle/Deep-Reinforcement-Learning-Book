# Ch5. 深層強化学習DQNを実装しよう

## 深層学習DQNの解説

### 強化学習のおさらい

- ある状態($s$)である行動($a$)をした時の報酬を最大化できるように行動を選択したい。
- 今回の状態($s_t$)と行動($a_t$)を決め時に得られる報酬($Q(s_t, a_t)$)を、即時得る報酬($R_{t+1}$)と、割引率($\gamma$)で割り引いた次回以降に得られる報酬($\gamma\max_aQ(s_{t+1}, a)$)の加算値としてとえ、行動価値関数として定義する。
    - $Q(s_t, a_t) = R_{t+1}+\gamma\max_aQ(s_{t+1}, a)$ 
- 正しい行動価値関数が分かれば、正しい行動を選択できるが、最初は行動価値関数に誤差(TD誤差)がある。
    - $TD誤差 = R_{t+1}+\gamma\max_aQ(s_{t+1}, a)-Q(s_t, a_t)$
- 正しい行動価値関数を明らかにするため、行動を$\epsilon$-greedy法で選択しながら、行動価値関数を更新する。
    - $Q(s_t, a_t) = Q(s_t, a_t) +  \eta(R_{t+1}+\gamma\max_aQ(s_{t+1}, a)-Q(s_t, a_t))$


### 表形式の強化学習の課題：状態変数が多くなると学習が難しい

- 表形式表現の概要：行動価値Q(s, a)
    - 行番号：エージェントの状態(s)。連続値の場合は、離散化が必要。
    - 列番号：エージェントの行動(a)
- 状態や行動を細かく区切ると、学習すべき行動価値の数が多くなり、必要な試行回数が膨大になってしまう。

### 深層強化学習の解説：DNNで行動価値関数を表現

- DNNの入出力
    - 入力：各状態変数の値($s_t$)
        - 状態変数の数(位置、速度など)だけ、入力の次元数を設定する。
        - 離散化が不要。
    - 出力：行動種類毎の行動価値($Q(s_t, a_t)$)
- DNNの更新式
    - 回帰問題として、$Q(s_t, a_t)$を求める。
    - 誤差関数として、TD誤差を2乗した値を使う。
        - $E(s_t, a_t)=(R_{t+1}+\gamma\max_aQ(s_{t+1}, a)-Q(s_t, a_t))^2$
        - なお、$\max_aQ(s_{t+1}, a)$は、DNNに$s_{t+1}$を入力して求める。

## DQNの実装に重要な4つの工夫

- Experience Replayによる学習
    - 行動選択結果を順次学習させるのではなく、行動選択を予めまとめて実行し、ランダムに複数ステップを選択して学習させる
    - 理由：順次学習させると、報酬が似通ってしまい、学習が不安定になるため。
- Fixed Target Q-Network：TD誤差算出用に過去パラメータの行動価値関数ネットワーク(fixed target network)を使う
    - 理由：パラメータを更新するネットワーク(main network)と、TD誤差算出用に$\max_aQ(s_{t+1}, a)$を算出するネットワークを同じにすると、学習が不安定になるため。
    - なお、target networkは、定期的にmain networkで更新する。
- 報酬のclipping
    - 即時報酬を、-1, 0, 1のいずれかに固定する。
    - 理由：課題を変えても、同じハイパーパラメータで学習させたいため。
- Huber関数を用いて、誤差を算出
    - Huber関数とは、$|誤差|>1$の時に、二乗値ではなく絶対値を算出する関数
    - 理由：二乗誤差では、誤差が大きくなった時に学習が不安定になるため。

## DQNの実装(1)：省略

## DQNの実装(2)：省略